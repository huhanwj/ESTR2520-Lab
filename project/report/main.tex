\documentclass[12pt]{ftec2101} 
% This template is modified from COLT 2020

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e

\title{Course Project: Portfolio Optimization in Practice}
\usepackage{times}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{enumitem}
% Use \Name{Author Name} to specify the name.
% If the surname contains spaces, enclose the surname
% in braces, e.g. \Name{John {Smith Jones}} similarly
% if the name has a "von" part, e.g \Name{Jane {de Winter}}.
% If the first letter in the forenames is a diacritic
% enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

% Two authors with the same address
% \coltauthor{\Name{Author Name1} \Email{abc@sample.com}\and
%  \Name{Author Name2} \Email{xyz@sample.com}\\
%  \addr Address}

% Three or more authors with the same address:
% \coltauthor{\Name{Author Name1} \Email{an1@sample.com}\\
%  \Name{Author Name2} \Email{an2@sample.com}\\
%  \Name{Author Name3} \Email{an3@sample.com}\\
%  \addr Address}
\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand{\vect}[1]{\mathbf{#1}}
% Authors with different addresses:
\coltauthor{%
 \Name{HU, Han} \Email{hanhu@link.cuhk.edu.hk}\\
 \addr Department of Information Engineering, CUHK
% \AND
% \Name{Author Name2} \Email{xyz@sample.com}\\ % uncomment this if you are working in a group of 2
% \addr Address 2%
}

\begin{document}

\maketitle


\section{Introduction}

This is where the content of your paper goes.

\section{Task 1}
\subsection{Subquestion (a)}
Derivation of the KKT conditions for the (simplified) Markowitz's mean-variance Portfolio Optimization problem stated in formula (1.1) in project specfication is:
\begin{align}
    L(p,\boldsymbol{\lambda}) = \frac{1}{2}\vect{p}^T\boldsymbol{\Sigma} \vect{p} + \lambda_{1} (\vect{1}^T \vect{p}-B) +\lambda_{2} (\bar{\vect{r}}^T \vect{p}-R_{d})\ ,
\end{align}
where $B$ is the fixed budget, $R_d$ is the fixed desired return and $\vect{1}$ is an all-one vector according to the project specfication.

\noindent
For the corvariance matrix, we have $\boldsymbol{\Sigma}^T = \boldsymbol{\Sigma}$. Then $\nabla \left(\frac{1}{2}\vect{p}^T \boldsymbol{\Sigma} \vect{p}\right) = \frac{1}{2}(\boldsymbol{\Sigma}\vect{p}+\boldsymbol{\Sigma}^T \vect{p}) = \boldsymbol{\Sigma}\vect{p}$. 

\noindent
Solving the KKT condition, we have
\begin{align}
    \begin{cases}
        \nabla_{p} L(p,\boldsymbol{\lambda}) = 0 \\
        \nabla_{\lambda} L(p,\boldsymbol{\lambda}) = 0
    \end{cases}
    \implies
    \begin{cases}
        \boldsymbol{\Sigma}\vect{p}+\lambda_1 \vect{1} + \lambda_2 \bar{\vect{r}} = 0 \\
        \vect{1}^T \vect{p} - B = 0 \\
        \bar{\vect{r}}^T \vect{p} - R_d = 0
    \end{cases}
    \label{KKT:1}
\end{align}

\noindent
From the task specfication, we know that the optimal $\vect{p}^{*}$ is given as
\begin{align}
    \vect{p}^{*} = \frac{\boldsymbol{\Sigma}^{-1}\{(r_0 B-r_1 R_d)\vect{1}+(r_2 R_d- r_1 B)\bar{\vect{r}}\}}{r_0 r_2 - r_1^2} \ ,
    \label{p:optimal}
\end{align}
where $r_0 = \bar{\vect{r}}^T \boldsymbol{\Sigma}^{-1} \bar{\vect{r}}$, $r_1 = \vect{1}^T \boldsymbol{\Sigma}^{-1} \bar{\vect{r}}$, $r_2 = \vect{1}^T \boldsymbol{\Sigma}^{-1} \vect{1}$.

\noindent
By plugging (\ref{p:optimal}) into the three equations in (\ref{KKT:1}), we get
\begin{align}
    \boldsymbol{\Sigma} \vect{p}^{*} +\lambda_1 \vect{1} + \lambda_2 \bar{\vect{r}}&= \frac{\boldsymbol{\Sigma}\boldsymbol{\Sigma}^{-1}\{(r_0 B - r_1 R_d)\vect{1}+(r_2 R_d - r_1 B)\bar{\vect{r}}\}}{r_0 r_2 -r_1^2} + \lambda_1 \vect{1} + \lambda_2 \bar{\vect{r}} \\
    &= \frac{r_0 B-r_1 R_d}{r_0 r_2 -r_1^2}\vect{1}+\frac{r_2 R_d - r_1 B}{r_0 r_2 -r_1^2}\bar{\vect{r}} + \lambda_1 \vect{1} +\lambda_2 \bar{\vect{r}}\ .
    \label{KKT:in:1}
\end{align}
\begin{align}
    \vect{1}^T \vect{p}^{*} - B &= \frac{\vect{1}^T\boldsymbol{\Sigma}^{-1}\{(r_0 B-r_1 R_d)\vect{1}+(r_2 R_d - r_1 B)\bar{\vect{r}}\}}{r_0 r_2 - r_1^2}- B \\
    & = \frac{\vect{1}^T\boldsymbol{\Sigma}^{-1}r_0 B \vect{1}-\vect{1}^T\boldsymbol{\Sigma}^{-1}r_1 R_d \vect{1}+\vect{1}^T\boldsymbol{\Sigma}^{-1}r_2 R_d \bar{\vect{r}}-\vect{1}^T \boldsymbol{\Sigma}^{-1} r_1 B \bar{\vect{r}}}{r_0 r_2 -r_1^2} - B \\
    & = \frac{r_2 r_0 B - r_2 r_1 R_d + r_1 r_2 R_d -r_1^2 B}{r_0 r_2 -r_1^2} - B \\
    & = B - B \\
    & = 0
    \label{KKT:in:2}
\end{align}
\begin{align}
    \bar{\vect{r}}^T \vect{p}^{*} -R_d &= \frac{\bar{\vect{r}}^T\boldsymbol{\Sigma}^{-1}\{(r_0 B-r_1 R_d)\vect{1}+(r_2 R_d - r_1 B)\bar{\vect{r}}\}}{r_0 r_2 - r_1^2}- R_d \\
    &= \frac{\bar{\vect{r}}^T\boldsymbol{\Sigma}^{-1}r_0 B \vect{1}-\bar{\vect{r}}^T\boldsymbol{\Sigma}^{-1}r_1 R_d \vect{1}+\bar{\vect{r}}^T\boldsymbol{\Sigma}^{-1}r_2 R_d \bar{\vect{r}}-\bar{\vect{r}}^T \boldsymbol{\Sigma}^{-1} r_1 B \bar{\vect{r}}}{r_0 r_2 -r_1^2} - R_d \ .
    \label{KKT:in:3:half}
\end{align}
Here we first calculate $\bar{\vect{r}}^T \boldsymbol{\Sigma}^{-1} \vect{1}$,
\begin{align}
    (\bar{\vect{r}}^T \boldsymbol{\Sigma}^{-1} \vect{1})^T = (\boldsymbol{\Sigma}^{-1} \vect{1})^T \bar{\vect{r}} = \vect{1} \boldsymbol{\Sigma}^{-1} \bar{\vect{r}} = r_1 \ .
    \label{KKT:in:3:com}
\end{align}
Combing (\ref{KKT:in:3:half}) and (\ref{KKT:in:3:com}), we obtain
\begin{align}
    \bar{\vect{r}}^T \vect{p}^{*} -R_d &= \frac{r_1 r_0 B - r_1^2 R_d + r_0 r_2 R_d - r_0 r_1 B}{r_0 r_2 - r_1^2} - R_d \\
    &= R_d - R_d \\ 
    &= 0
    \label{KKT:in:3}
\end{align}
\noindent
From (\ref{KKT:in:1}), (\ref{KKT:in:2}) and (\ref{KKT:in:3}), by choosing $\lambda_1 = \frac{r_1 R_d- r_0 B}{r_0 r_2 -r_1^2}$, $\lambda_2 = \frac{r_1 B - r_2 R_d}{r_0 r_2 -r_1^2}$, all three equations equal to $0$ and $\vect{p}^{*}$ is proven to be a valid optimal solution for (1.1).
\subsection{Subquestion (b)}
From the simplified conditions given in the specfication and calculations based on the additional conditions, we have 
\begin{align*}
    \bar{\vect{r}} =
    \begin{pmatrix}
        \bar{r}_1 \\
        1
    \end{pmatrix}\ ,
    r_0 = \bar{r}_1^2+1\ ,
    r_1 = \bar{r}_1 + 1\ , 
    r_2 = 2\ ,
    \boldsymbol{\Sigma} = \boldsymbol{\Sigma}^{-1} =
    \begin{pmatrix}
        1 & 0 \\
        0 & 1
    \end{pmatrix}
\end{align*}

\noindent
Plug the above additional conditions into the given $\vect{p}^{*}$ given in subsection (a), we get
\begin{align}
    \vect{p}^{*} &= \frac{\boldsymbol{\Sigma}^{-1}\{[(\bar{r}_1^2+1)B-(\bar{r}_1+1)R_d]\vect{1}+[2R_d - (\bar{r}_1 + 1)B]\bar{\vect{r}}\}}{2\bar{r}_1^2 + 2 - (\bar{r}_1+1)^2} \\
    &= \frac{1}{\bar{r}_1 - 1} \cdot
    \begin{pmatrix}
        R_d - B \\
        B\bar{r}_1 - R_d
    \end{pmatrix}\\
    &=
    \begin{pmatrix}
        \frac{R_d-B}{\bar{r}_1 - 1} \\
        B+ \frac{B-R_d}{\bar{r}_1 - 1}
    \end{pmatrix}\ .
    \label{optimal:p}
\end{align}
By observing (\ref{optimal:p}), we can conclude that
\begin{enumerate}
    \item If $\bar{r}_1$ increases, since $R_d > B$, $p_1^{*}$ will decrease, $p_2^{*}$ will increase.
    \item If $R_d$ increases, $p_1^{*}$ will increase, $p_2^{*}$ will decrease.
\end{enumerate}

\section{Task 2}
The mixed integer program formed is
\begin{gather}
    \min_{\vect{x}\in\mathbb{R}^n ,\  \vect{y}\in \mathbb{Z}^n} \quad (\vect{x}+\boldsymbol{\omega})^T \boldsymbol{\Sigma} (\vect{x}+\boldsymbol{\omega}) \\
    \label{mix:integer}
    \begin{aligned}
    \textup{s.t.} \quad \sum_{i=1}^{n} (x_i+\omega_i)\bar{r}_i \geq R_d\\
                \sum_{i=1}^{n} (x_i+y_i c) \leq B\\
                -M y_i \leq x_i \leq M y_i,\ i=1,\cdots,n\\
                x_i \geq -\omega_i,\ i = 1,\cdots,n\\
                y_i \in \{0,1\},\ i = 1,\cdots,n
\end{aligned}
\end{gather}
\section{Task 3}
\subsection{Subquestion (a)}
We first define a new vect $\vect{x'}$ as
\begin{align*}
    \vect{x'} = 
    \begin{pmatrix}
        t \\
        \vect{x}
    \end{pmatrix}\ , 
\end{align*}
where $\vect{x} \in \mathbb{R}^n$, $t \in \mathbb{R}$.

\noindent
The nonlinear program formed is 
\begin{gather}
    \min_{\vect{x'} \in \mathbb{R}^{n+1}} \quad
    \begin{pmatrix}
        1 \\
        0 \\
        \vdots \\
        0
    \end{pmatrix}^T
    \cdot \vect{x'}\\
\begin{aligned}
    \textup{s.t.} \quad \sum_{i=1}^{n} (x_i + \omega_i)\bar{r}_i \geq R_d \\
                        \sum_{i=1}^{n} (x_i + a x_i^2) \leq B \\
                        t \geq \sqrt{(\vect{x}+\boldsymbol{\omega})^T \boldsymbol{\Sigma} (\vect{x}+\boldsymbol{\omega})} \\
                        -M \leq x'_i \leq M,\ i=2,\cdots,n+1 \\
                        x'_i \geq - \omega_i - 1,\ i=2,\cdots,n+1
\end{aligned}
\label{nonlinear:constraint}
\end{gather}
\subsection{Subquestion (b)}
To write the nonlinear program as an SOCP, we need to modify the constraints in (\ref{nonlinear:constraint}) as
\begin{align}
    \sum_{i=1}^{n} (x_i + \omega_i)\bar{r}_i \geq R_d \implies \left \Vert 0 \vect{x'} + 
    \begin{pmatrix}
        R_d\\
        0\\
        \vdots\\
        0
    \end{pmatrix} \right \Vert \leq
    \begin{pmatrix}
        0\\
        \bar{\vect{r}}
    \end{pmatrix}^T \vect{x'} + \bar{\vect{r}}^T \boldsymbol{\omega}
\end{align}
\begin{align}
    \sum_{i=1}^{n} (x_i + a x_i^2) \leq B &\implies \sum_{i=1}^{n} \left(x_i + a x_i^2+\frac{1}{4a}\right) \leq B+\frac{n}{4a} \\
    &\implies \left \Vert \sqrt{a} \vect{x} + \frac{1}{2\sqrt{a}} \vect{1}\right \Vert \leq \sqrt{B+\frac{n}{4a}}
\end{align}
\begin{align}
    t \geq \sqrt{(\vect{x}+\boldsymbol{\omega})^T \boldsymbol{\Sigma} (\vect{x}+\boldsymbol{\omega})} &\implies \left \Vert \boldsymbol{\Sigma}^{\frac{1}{2}} \vect{x} + \boldsymbol{\Sigma}^{\frac{1}{2}}\boldsymbol{\omega}\right \Vert \leq t \\
    &\implies \left \Vert
    \begin{pmatrix}
        0 & \\
        \vdots & \boldsymbol{\Sigma}^{\frac{1}{2}}\\
        0 & 
    \end{pmatrix} \vect{x'} + \boldsymbol{\Sigma}^{\frac{1}{2}} \boldsymbol{\omega} \right \Vert \leq
    \begin{pmatrix}
        1 \\
        0 \\
        \vdots \\
        0
    \end{pmatrix}^T \vect{x'}
\end{align}
Thus, the formed SOCP is 
\begin{gather}
    \min_{\vect{x'} \in \mathbb{R}^{n+1}} \quad \begin{pmatrix}
        1 \\
        0 \\
        \vdots \\
        0
    \end{pmatrix}^T
    \cdot \vect{x'}\\
    \begin{aligned}
    \textup{s.t.} \quad  \left \Vert 0 \vect{x'} + 
    \begin{pmatrix}
        R_d\\
        0\\
        \vdots\\
        0
    \end{pmatrix} \right \Vert \leq
    \begin{pmatrix}
        0\\
        \bar{\vect{r}}
    \end{pmatrix}^T \vect{x'} + \bar{\vect{r}}^T \boldsymbol{\omega}\\
    \left \Vert \sqrt{a} \vect{x} + \frac{1}{2\sqrt{a}} \vect{1}\right \Vert \leq \sqrt{B+\frac{n}{4a}}\\
    \left \Vert
    \begin{pmatrix}
        0 & \\
        \vdots & \boldsymbol{\Sigma}^{\frac{1}{2}}\\
        0 & 
    \end{pmatrix} \vect{x'} + \boldsymbol{\Sigma}^{\frac{1}{2}} \boldsymbol{\omega} \right \Vert \leq
    \begin{pmatrix}
        1 \\
        0 \\
        \vdots \\
        0
    \end{pmatrix}^T \vect{x'}\\
    -M \leq x'_i \leq M,\ i=2,\cdots,n+1 \\
    x'_i \geq - \omega_i - 1,\ i=2,\cdots,n+1
    \end{aligned}
\end{gather}
% It is a good habit to organize your reference in a bibliography (.bib) file. 
\bibliography{estr_bib}

\appendix

\section{My Proof of Theorem 1}

This is a boring technical proof.
 

\end{document}
